{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside Movies\n",
    "\n",
    "Analysing movies data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes = []\n",
    "edges = []\n",
    "\n",
    "def get_imdb_id(href_text):\n",
    "    m = re.search('(?:[\\w]*[\\d]{7})', href_text)\n",
    "\n",
    "    if m:\n",
    "        return m.group(0)\n",
    "    \n",
    "def get_imdb_soup(url):\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "def get_plot_data(soup, url_base, key):\n",
    "    item_tags = soup.find_all('span', itemprop = key, itemtype = 'http://schema.org/Person')\n",
    "    return [{\n",
    "        'id': get_imdb_id(tag.find_all('a')[0].get('href')),\n",
    "        'name': tag.find_all('span', itemprop='name')[0].text,\n",
    "        'url': url_base + tag.find_all('a')[0].get('href')\n",
    "    } for tag in item_tags]\n",
    "\n",
    "def add_node(id_, label, group, image = None):\n",
    "    search = [node for node in nodes if node['id'] == id_]\n",
    "    \n",
    "    if(len(search) == 0):\n",
    "        node = {\n",
    "            'id': id_,\n",
    "            'label': label,\n",
    "            'group': group\n",
    "        }\n",
    "        if image is not None:\n",
    "            node['shape'] = 'circularImage'\n",
    "            node['image'] = image\n",
    "            node['size']  = 20\n",
    "        nodes.append(node)\n",
    "        return node\n",
    "    else:\n",
    "        return search[0]\n",
    "\n",
    "def add_edge(node_from, node_to, label):\n",
    "    search = [edge for edge in edges if (edge['from'] == node_from['id'] and edge['to'] == node_to['id'] and edge['label'] == label)]\n",
    "    \n",
    "    if(len(search) == 0):\n",
    "        edges.append({ 'from': node_from['id'], 'to': node_to['id'], 'label': label })\n",
    "\n",
    "def parse_person(imdb_id, max_depth, base_node = None, current_depth = 1):\n",
    "    if(current_depth > max_depth):\n",
    "        return\n",
    "\n",
    "    base_url = 'http://www.imdb.com/name/'\n",
    "    soup = get_imdb_soup(base_url + imdb_id)\n",
    "    \n",
    "    image = soup.find('td', id='img_primary').find('img')\n",
    "    if image is not None:\n",
    "        image = image.get('src')\n",
    "        base_node['shape'] = 'circularImage'\n",
    "        base_node['image'] = image\n",
    "        base_node['size']  = 20\n",
    "    \n",
    "    knowfor_tags = soup.select('div#knownfor div.knownfor-title')\n",
    "    movies = [{\n",
    "        'id': get_imdb_id(movie_tag.find('div', class_ = 'knownfor-title-role').find('a').get('href')),\n",
    "        'name': movie_tag.find('div', class_ = 'knownfor-title-role').find('a').text.strip(),\n",
    "        'url': movie_tag.find('div', class_ = 'knownfor-title-role').find('a').get('href'),\n",
    "        'image': movie_tag.find('img').get('src') if movie_tag.find('img') else None\n",
    "    } for movie_tag in knowfor_tags]\n",
    "\n",
    "    for movie in movies:\n",
    "        node = add_node(movie['id'], movie['name'], 'Movie', movie['image'])\n",
    "        add_edge(node, base_node, 'KF')\n",
    "        parse_title(imdb_id = movie['id'], max_depth = max_depth, base_node = node, current_depth = current_depth + 1)\n",
    "    \n",
    "def parse_title(imdb_id, max_depth, base_node = None, current_depth = 1):\n",
    "    if(current_depth > max_depth):\n",
    "        return\n",
    "\n",
    "    base_url = 'http://www.imdb.com/title/'\n",
    "    soup = get_imdb_soup(base_url + imdb_id)\n",
    "    \n",
    "    if(not base_node):\n",
    "        title_tags = soup.find_all('h1', itemprop='name')[0]\n",
    "        title = title_tags.text\n",
    "        year = title_tags.select('span#titleYear')[0].text\n",
    "        movie = title.replace(year, '').strip()\n",
    "        movie_image = soup.find('div', class_ = 'poster')\n",
    "        if movie_image:\n",
    "            movie_image = movie_image.find('img').get('src')\n",
    "        \n",
    "        base_node = add_node(imdb_id, movie, 'Movie', movie_image)\n",
    "\n",
    "    directors = get_plot_data(soup, base_url, 'director')\n",
    "    writers = get_plot_data(soup, base_url, 'creator')\n",
    "    stars = get_plot_data(soup, base_url, 'actors')\n",
    "    genres = soup.select('.title_wrapper')[0].find_all('a', href = re.compile('/genre/'))\n",
    "    genres = [{\n",
    "        'id': genre.get('href').split('?ref')[0],\n",
    "        'name': genre.text,\n",
    "        'url': base_url + genre.get('href')\n",
    "    } for genre in genres]\n",
    "\n",
    "    for genre in genres:\n",
    "        add_edge(add_node(genre['id'], genre['name'], 'Genre'), base_node, 'G')\n",
    "\n",
    "    def add_person_items(items, label):\n",
    "        for item in items:\n",
    "            node = add_node(item['id'], item['name'], 'Person')\n",
    "            add_edge(node, base_node, label)\n",
    "\n",
    "            parse_person(\n",
    "                imdb_id = item['id'],\n",
    "                max_depth = max_depth,\n",
    "                base_node = node,\n",
    "                current_depth = current_depth + 1\n",
    "            )\n",
    "\n",
    "    add_person_items(directors, 'D')\n",
    "    add_person_items(writers, 'W')\n",
    "    add_person_items(stars, 'S')\n",
    "        \n",
    "def parse_entry(imdb_url, max_depth = 3):\n",
    "    imdb_id = get_imdb_id(imdb_url)\n",
    "\n",
    "    if 'tt' in imdb_id:\n",
    "        parse_title(imdb_id, max_depth = max_depth)\n",
    "    elif 'nm' in imdb_id:\n",
    "        parse_person(imdb_id, max_depth = max_depth)\n",
    "\n",
    "    print('Data is loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "search_input = widgets.Text(placeholder='Pulp Fiction...')\n",
    "index_input = widgets.Text(placeholder='1...')\n",
    "label_results = widgets.HTML(value='')\n",
    "\n",
    "box_search = widgets.VBox([\n",
    "    widgets.Label(value=\"Search movie:\"),\n",
    "    search_input,\n",
    "    label_results\n",
    "])\n",
    "\n",
    "display(box_search)\n",
    "\n",
    "titles = []\n",
    "title = None\n",
    "\n",
    "def handle_search_submit(sender):\n",
    "    label_results.value = 'Loading...'\n",
    "    \n",
    "    query_token = '+'.join(search_input.value.split(' '))\n",
    "    search_url = 'http://www.imdb.com/find?q={}&s=all'.format(query_token)\n",
    "    page = requests.get(search_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    sections = soup.find_all('div', class_='findSection')\n",
    "\n",
    "    has_titles = lambda tag : 'Titles' in tag.find('h3', class_='findSectionHeader').text\n",
    "    section = [section for section in sections if has_titles(section)]\n",
    "\n",
    "    if len(section) > 0:\n",
    "        result_list = section[0].find('table', class_='findList').find_all('td', class_='result_text')\n",
    "\n",
    "    titles_html = ''\n",
    "\n",
    "    for item in result_list:\n",
    "        titles.append({\n",
    "            'url': item.find('a').get('href'),\n",
    "            'text': item.text\n",
    "        })\n",
    "        titles_html += '<b>{0}</b> - {1}<br/>'.format(len(titles), item.text)\n",
    "    \n",
    "    label_results.value = titles_html\n",
    "    \n",
    "    box_index = widgets.VBox([\n",
    "        widgets.Label(value=\"Type the index of the desired movie to analyze:\"),\n",
    "        index_input\n",
    "    ])\n",
    "    display(box_index)\n",
    "\n",
    "    \n",
    "def handle_index_submit(sender):  \n",
    "    index = int(index_input.value) - 1\n",
    "    title = titles[index]\n",
    "    print('Loading data for: ' + title['text'])\n",
    "    parse_entry(title['url'])\n",
    "\n",
    "search_input.on_submit(handle_search_submit)\n",
    "index_input.on_submit(handle_index_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"imdb-movie-network\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "import json\n",
    "\n",
    "# Increase the size of the first node\n",
    "if 'size' in nodes[0]:\n",
    "    nodes[0]['size'] *= 2\n",
    "\n",
    "# Transform the graph into a JSON graph\n",
    "data = { 'nodes': nodes, 'edges': edges }\n",
    "jsonGraph = json.dumps(data, indent = 4)\n",
    "\n",
    "# Clear nodes and edges\n",
    "nodes = []\n",
    "edges = []\n",
    "\n",
    "# Send to Javascript\n",
    "Javascript(\"\"\"window.jsonGraph={};\"\"\".format(jsonGraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "requirejs.config({\n",
    "    paths: {\n",
    "        vis: 'vis'\n",
    "    }\n",
    "});\n",
    "\n",
    "require(['vis'], function(vis){\n",
    "    var container = document.getElementById('imdb-movie-network');\n",
    "    var options = {\n",
    "        width: '900px',\n",
    "        height: '500px',\n",
    "        nodes: {\n",
    "            shape: 'dot',\n",
    "            size: 10,\n",
    "            borderWidth: 2\n",
    "        },\n",
    "        edges: {\n",
    "            font: {\n",
    "                size: 12\n",
    "            }\n",
    "        }\n",
    "    };\n",
    "    \n",
    "    // Load the JSON graph we generated from IPython input\n",
    "    var graph = window.jsonGraph;\n",
    "    \n",
    "    // Display Graph\n",
    "    var network = new vis.Network(container, graph, options);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tweepy\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Para conseguir essas chaves da API do Twitter, acessar: https://apps.twitter.com/ e criar um app\n",
    "consumer_key = 'tkKbSHx4YP5lEzhGjndBlT0Z1'\n",
    "consumer_secret = 'UOEG4egOiAVk3ZgcNrzI8Rn8GzBaeTFsxP9AQtwXMTqHdVOSgb'\n",
    "\n",
    "access_token = '117218938-V27a0owN7LKEH23vJx5nAJ6QvttsANGETc2oyeAo'\n",
    "access_token_secret = '0PLzG7TAX1eqmkXg90JtWaR4b5UZZIsDRKOMjRdnKEjTI'\n",
    "\n",
    "# Autenticação\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definir o que será procurado no Twitter\n",
    "public_tweets = api.search(q='Wonder Woman', count = 10000)\n",
    "\n",
    "# Array de manipulação\n",
    "polarity = [] # np.array()\n",
    "\n",
    "# Percorrer tweets\n",
    "for tweet in public_tweets:\n",
    "    # print(tweet.text)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    polarity.append(analysis.sentiment.polarity)\n",
    "    \n",
    "# Converte array normal para array numpy\n",
    "polarity = np.array(polarity)\n",
    "\n",
    "# Média\n",
    "mean = np.mean(polarity)\n",
    "print('Mean: {0}'.format(mean))\n",
    "\n",
    "# Média desconsiderando os valores zeros\n",
    "nonzero_indexs = np.nonzero(polarity)\n",
    "mean_nonzero = np.mean(polarity[nonzero_indexs])\n",
    "print('Mean (disregarding zero values): {0}'.format(mean_nonzero))\n",
    "\n",
    "# Verifica se o filme é favorável, não favorável e neutro\n",
    "if mean > 0 :\n",
    "    print('O filme é favorável!')\n",
    "elif mean < 0 :\n",
    "    print('O filme não é favorável!')\n",
    "else :\n",
    "    print('O filme é neutro!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "10006e1890a749d2a5c8217df84d558c": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "daef55927d8a40ca992f36b26d7ebf12": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
